{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4da38ad",
   "metadata": {},
   "source": [
    "# 1. Libraries & Sample Data\n",
    "The first step is to load our Python Libraries and download the sample data. The dataset represents Apple stock price (1d bars) for the year 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b467b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa05430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Python Libraries\n",
    "import math\n",
    "import keras\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# for dataframe display\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "def display_df(df):\n",
    "    # Puts the scrollbar next to the DataFrame\n",
    "    display(HTML(\"<div style='height: 200px; overflow: auto; width: fit-content'>\" + df.to_html() + \"</div>\"))\n",
    "\n",
    "# for reproducability of answers\n",
    "keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa831031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Sample Data AAPL_2009_4m_training_features_1d (cleaned, not normalized, with features)\n",
    "data = pd.read_csv('AAPL_2009_4m_training_features_1d.csv')\n",
    "# track index to remember which feature is which\n",
    "idx_close = 0\n",
    "idx_bb_upper = 1\n",
    "idx_bb_lower = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8526a",
   "metadata": {},
   "source": [
    "# 2. Train / Test Split\n",
    "Now that we have loaded our cleaned price dataset, we are ready to feed the data into our model. With this in mind, we select Close as our singular training feature, and split the data ito train and test data (80/20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7557879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset df into train (80%) and test (20%) datasets\n",
    "training_rows = int(len(data.index)*0.8)\n",
    "train_df = data.loc[:training_rows].set_index(\"Date\")\n",
    "test_df = data.loc[training_rows+1:].set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1dca49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='height: 200px; overflow: auto; width: fit-content'><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>BB_lower</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-04-30</th>\n",
       "      <td>4.493929</td>\n",
       "      <td>4.576559</td>\n",
       "      <td>4.040905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-01</th>\n",
       "      <td>4.544286</td>\n",
       "      <td>4.587013</td>\n",
       "      <td>4.082345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-04</th>\n",
       "      <td>4.544286</td>\n",
       "      <td>4.606655</td>\n",
       "      <td>4.102880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-05</th>\n",
       "      <td>4.739643</td>\n",
       "      <td>4.677963</td>\n",
       "      <td>4.082501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-06</th>\n",
       "      <td>4.732143</td>\n",
       "      <td>4.732943</td>\n",
       "      <td>4.077699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-07</th>\n",
       "      <td>4.609286</td>\n",
       "      <td>4.745322</td>\n",
       "      <td>4.110821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-08</th>\n",
       "      <td>4.613929</td>\n",
       "      <td>4.763748</td>\n",
       "      <td>4.126752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-11</th>\n",
       "      <td>4.627500</td>\n",
       "      <td>4.781972</td>\n",
       "      <td>4.141921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-12</th>\n",
       "      <td>4.627500</td>\n",
       "      <td>4.789788</td>\n",
       "      <td>4.174320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-13</th>\n",
       "      <td>4.267500</td>\n",
       "      <td>4.781611</td>\n",
       "      <td>4.189103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-14</th>\n",
       "      <td>4.391071</td>\n",
       "      <td>4.763703</td>\n",
       "      <td>4.225975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-15</th>\n",
       "      <td>4.372143</td>\n",
       "      <td>4.764809</td>\n",
       "      <td>4.221298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-18</th>\n",
       "      <td>4.523214</td>\n",
       "      <td>4.760894</td>\n",
       "      <td>4.247178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-19</th>\n",
       "      <td>4.551786</td>\n",
       "      <td>4.761043</td>\n",
       "      <td>4.267350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-20</th>\n",
       "      <td>4.551786</td>\n",
       "      <td>4.757918</td>\n",
       "      <td>4.291689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-21</th>\n",
       "      <td>4.435000</td>\n",
       "      <td>4.758358</td>\n",
       "      <td>4.286892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-22</th>\n",
       "      <td>4.375000</td>\n",
       "      <td>4.761653</td>\n",
       "      <td>4.273240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-26</th>\n",
       "      <td>4.670714</td>\n",
       "      <td>4.779766</td>\n",
       "      <td>4.276734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-27</th>\n",
       "      <td>4.751786</td>\n",
       "      <td>4.809941</td>\n",
       "      <td>4.279238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-28</th>\n",
       "      <td>4.823929</td>\n",
       "      <td>4.852703</td>\n",
       "      <td>4.271940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>4.823929</td>\n",
       "      <td>4.889624</td>\n",
       "      <td>4.268019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-01</th>\n",
       "      <td>4.976786</td>\n",
       "      <td>4.957827</td>\n",
       "      <td>4.243066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-02</th>\n",
       "      <td>4.981786</td>\n",
       "      <td>5.016855</td>\n",
       "      <td>4.227788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-03</th>\n",
       "      <td>5.033929</td>\n",
       "      <td>5.070065</td>\n",
       "      <td>4.204006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-04</th>\n",
       "      <td>5.133571</td>\n",
       "      <td>5.142719</td>\n",
       "      <td>4.171496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-05</th>\n",
       "      <td>5.166786</td>\n",
       "      <td>5.220477</td>\n",
       "      <td>4.149488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-08</th>\n",
       "      <td>5.166786</td>\n",
       "      <td>5.288251</td>\n",
       "      <td>4.136999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-09</th>\n",
       "      <td>5.097143</td>\n",
       "      <td>5.334960</td>\n",
       "      <td>4.137254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-10</th>\n",
       "      <td>5.008929</td>\n",
       "      <td>5.363685</td>\n",
       "      <td>4.146672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-11</th>\n",
       "      <td>4.998214</td>\n",
       "      <td>5.363575</td>\n",
       "      <td>4.219854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-12</th>\n",
       "      <td>4.891786</td>\n",
       "      <td>5.357769</td>\n",
       "      <td>4.275731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-15</th>\n",
       "      <td>4.860357</td>\n",
       "      <td>5.340136</td>\n",
       "      <td>4.342185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-16</th>\n",
       "      <td>4.860357</td>\n",
       "      <td>5.334018</td>\n",
       "      <td>4.382018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-17</th>\n",
       "      <td>4.842143</td>\n",
       "      <td>5.326406</td>\n",
       "      <td>4.418665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-18</th>\n",
       "      <td>4.852857</td>\n",
       "      <td>5.315920</td>\n",
       "      <td>4.459259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='height: 200px; overflow: auto; width: fit-content'><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>BB_lower</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-06-19</th>\n",
       "      <td>4.981429</td>\n",
       "      <td>5.287810</td>\n",
       "      <td>4.542011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-22</th>\n",
       "      <td>4.906071</td>\n",
       "      <td>5.214837</td>\n",
       "      <td>4.668092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-23</th>\n",
       "      <td>4.786071</td>\n",
       "      <td>5.200695</td>\n",
       "      <td>4.693770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-24</th>\n",
       "      <td>4.786071</td>\n",
       "      <td>5.197255</td>\n",
       "      <td>4.700638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-25</th>\n",
       "      <td>4.995000</td>\n",
       "      <td>5.199378</td>\n",
       "      <td>4.715622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-26</th>\n",
       "      <td>5.087143</td>\n",
       "      <td>5.210573</td>\n",
       "      <td>4.730748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-29</th>\n",
       "      <td>5.070357</td>\n",
       "      <td>5.219369</td>\n",
       "      <td>4.731310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-06-30</th>\n",
       "      <td>5.086786</td>\n",
       "      <td>5.229668</td>\n",
       "      <td>4.731510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display train and test dfs (ensure no overlap)\n",
    "display_df(train_df)\n",
    "display_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a6ed95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert train and test dfs to np arrays with dtype=float\n",
    "X_train = train_df.values.astype(float)\n",
    "X_test = test_df.values.astype(float)\n",
    "# print the shape of X_train to remind yourself how many examples and features are in the dataset\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c77a8",
   "metadata": {},
   "source": [
    "# 3. Define the Agent\n",
    "Now that our data is ready to use, we can define the Reinforcement Learning Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd06b6d",
   "metadata": {},
   "source": [
    "### Define the DQN Model\n",
    "The first step in defining our agent is the Deep Q-Network model definition. For this excercise, we are creating a sequential model with three layers. The first two layers have output shape of 32 and 8, respectively, and a RELU activation. The output layer has an output shape of the size of our action space (buy, sell, hold), and a linear activation. Our Loss function is Mean Squared Error, and our optimizer is Adam with a learning rate of 0.001. Use Keras to build this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1e98591",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "# Define DQN Model Architecture\n",
    "class DQN(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "    \n",
    "        model = keras.models.Sequential()\n",
    "        #Input Layer\n",
    "        model.add(keras.layers.Dense(units=32, input_dim=state_size, activation=\"relu\"))\n",
    "        #Hidden Layer\n",
    "        model.add(keras.layers.Dense(units=8, activation=\"relu\"))\n",
    "        #Output Layer \n",
    "        model.add(keras.layers.Dense(action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "        self.model = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edabcd7d",
   "metadata": {},
   "source": [
    "### Define Agent Class\n",
    "Now that we have defined our underlying DQN Model, we must define out Reinforcement Learning Agent. The agent initialization is provided for you, you must define an act function, and an expereince replay function. As a reminder, the act function defines how our model will act (buy, hold, or sell) given a certain state. The Experience Replay function tackles catastrophic forgetting in our training process, by maintaining a memory buffer to allow training on independent / randomized minibatches of previous states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1489f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, window_size, num_features, test_mode=False, model_name=''):\n",
    "        self.window_size = window_size # How many days of historical data do we want to include in our state representation?\n",
    "        self.num_features = num_features # How many training features do we have?\n",
    "        self.state_size = window_size*num_features # State size includes number of training features per day, and number of lookback days \n",
    "        self.action_size = 3 # 0=hold, 1=buy, 2=sell\n",
    "        self.memory = deque(maxlen=1000) # Bound memory size: once the memory reaches 1000 units, the lefthand values are discarded as righthand values are added\n",
    "        self.inventory = [] # Inventory to hold trades\n",
    "        self.model_name = model_name # filename for saved model checkpoint loading\n",
    "        self.test_mode = test_mode # flag for testing (allows model load from checkpoint model_name)\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        self.model = keras.models.load_model(model_name) if test_mode else self._model()\n",
    "\n",
    "\n",
    "    #Deep Q Learning (DQL) model\n",
    "    def _model(self):\n",
    "        model = DQN(self.state_size, self.action_size).model\n",
    "        return model\n",
    "    \n",
    "\n",
    "    # DQL Predict (with input reshaping)\n",
    "    #   Input = State\n",
    "    #   Output = Q-Table of action Q-Values\n",
    "    def get_q_values_for_state(self, state):\n",
    "        return self.model.predict(state.flatten().reshape(1, self.state_size))\n",
    "    \n",
    "\n",
    "    # DQL Fit (with input reshaping)\n",
    "    #   Input = State, Target Q-Table \n",
    "    #   Output = MSE Loss between Target Q-Table and Actual Q-Table for State\n",
    "    def fit_model(self, input_state, target_output):\n",
    "        return self.model.fit(input_state.flatten().reshape(1, self.state_size), target_output, epochs=1, verbose=0)    \n",
    "    \n",
    "\n",
    "    # Agent Action Selector\n",
    "    #   Input = State\n",
    "    #   Policy = epsilon-greedy (to minimize possibility of overfitting)\n",
    "    #   Intitially high epsilon = more random, epsilon decay = less random later\n",
    "    #   Output = Action (0, 1, or 2)\n",
    "    def act(self, state): \n",
    "        # Choose any action at random (Probablility = epsilon for training mode, 0% for testing mode)\n",
    "        if not self.test_mode and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)   \n",
    "        # Choose the action which has the highest Q-value (Probablitly = 1-epsilon for training mode, 100% for testing mode)\n",
    "        options = self.get_q_values_for_state(state)\n",
    "        return np.argmax(options[0]) \n",
    "\n",
    "    # Experience Replay (Learning Function)\n",
    "    #   Input = Batch of (state, action, next_state) tuples\n",
    "    #   Optimal Q Selection Policy = Bellman equation\n",
    "    #   Important Notes = Model fitting step is in this function (fit_model)\n",
    "    #                     Epsilon decay step is in this function\n",
    "    #   Output = Model loss from fitting step\n",
    "    def exp_replay(self, batch_size):\n",
    "        losses = []\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "        for i in range(l - batch_size + 1, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "            \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            # reminders: \n",
    "            #   - state is a vector containing close & MA values for the current time step\n",
    "            #   - action is an integer representing the action taken by the act function at the current time step- buy, hold, or sell\n",
    "            #   - reward represents the profit of a given action - it is either 0 (for buy, hold, and sells which loose money) or the profit in dollars (for a profitable sell)\n",
    "            #   - next_state is a vector containing close & MA values for the next time step\n",
    "            #   - done is a boolean flag representing whether or not we are in the last iteration of a training episode (i.e. True when next_state does not exist.)\n",
    "            \n",
    "            if done:\n",
    "                # special condition for last training epoch in batch (no next_state)\n",
    "                optimal_q_for_action = reward  \n",
    "            else:\n",
    "                # target Q-value is updated using the Bellman equation: reward + gamma * max(predicted Q-value of next state)\n",
    "                optimal_q_for_action = reward + self.gamma * np.max(self.get_q_values_for_state(next_state))\n",
    "            # Get the predicted Q-values of the current state\n",
    "            target_q_table = self.get_q_values_for_state(state)  \n",
    "            # Update the output Q table - replace the predicted Q value for action with the target Q value for action \n",
    "            target_q_table[0][action] = optimal_q_for_action\n",
    "            # Fit the model where state is X and target_q_table is Y\n",
    "            history = self.fit_model(state, target_q_table)\n",
    "            losses += history.history['loss']\n",
    "\n",
    "        # define epsilon decay (for the act function)     \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0550bb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnlockwood/mambaforge/envs/ai_trading/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-11-25 13:15:34.593728: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-11-25 13:15:34.593748: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-11-25 13:15:34.593752: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-11-25 13:15:34.593771: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-25 13:15:34.593785: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m27\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">419</span> (1.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m419\u001b[0m (1.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">419</span> (1.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m419\u001b[0m (1.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# keras.utils.disable_interactive_logging()\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "window_size = 1\n",
    "agent = Agent(window_size, num_features=X_train.shape[1])\n",
    "# dot = keras.utils.model_to_dot(\n",
    "#     agent.model,\n",
    "#     show_shapes=True,\n",
    "#     show_dtype=True,\n",
    "#     show_layer_names=True,\n",
    "# )\n",
    "# dot.write(\"model.png\", format='png')\n",
    "# from IPython import display\n",
    "\n",
    "# display.Image('model.png')\n",
    "print(agent.model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee09a6",
   "metadata": {},
   "source": [
    "# 4. Train the Agent\n",
    "Now that our agent is defined, we are ready to train it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cd0f4",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Before we define the training loop, we will write some helper functions: one for printing price data, one to define the sigmoind funtion, one to grab the state representation, and one to plot the output of our trained model. The printing, sigmoid, and plotting functions are defined for you. You must define the function which gets the state representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "722fbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format price string\n",
    "def format_price(n):\n",
    "    return ('-$' if n < 0 else '$') + '{0:.2f}'.format(abs(n))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Plot behavior of trade output\n",
    "def plot_behavior(data_input, bb_upper_data, bb_lower_data, states_buy, states_sell, profit, train=True):\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.plot(data_input, color='k', lw=2., label= 'Close Price')\n",
    "    plt.plot(bb_upper_data, color='b', lw=2., label = 'Bollinger Bands')\n",
    "    plt.plot(bb_lower_data, color='b', lw=2.)\n",
    "    plt.plot(data_input, '^', markersize=10, color='r', label = 'Buying signal', markevery = states_buy)\n",
    "    plt.plot(data_input, 'v', markersize=10, color='g', label = 'Selling signal', markevery = states_sell)\n",
    "    plt.title('Total gains: %f'%(profit))\n",
    "    plt.legend()\n",
    "    if train:\n",
    "        plt.xticks(range(0, len(train_df.index.values), int(len(train_df.index.values)/15)), train_df.index.values[0:: int(len(train_df.index.values)/15)], rotation=45, fontsize='small')\n",
    "    else:\n",
    "        plt.xticks(range(0, len(test_df.index.values), int(len(test_df.index.values)/2)), test_df.index.values[0::int(len(test_df.index.values)/2)], rotation=45, fontsize='small')\n",
    "    plt.show()\n",
    "\n",
    "# Plot training loss\n",
    "def plot_losses(losses, title):\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('MSE Loss Value')\n",
    "    plt.xlabel('batch')\n",
    "    plt.show()\n",
    "\n",
    "# returns an an n-day state representation ending at time t\n",
    "def get_state(data, t, n): \n",
    "    # data is the dataset of interest which holds the state values (i.e. Close , BB Upper, BB Lower)\n",
    "    # t is the current time step \n",
    "    # n is the size of the training window \n",
    "\n",
    "    # the first step is to get the window of the dataset at the current time step (eg. if window size is 1, we grab the previous and the current time step)\n",
    "    # remember to define the special case for the first iteration, where there is no previous time step. See lesson X for a reminder of how to do this.\n",
    "\n",
    "    # once we have our state data, we need to apply the sigmoid to each feature.\n",
    "    # return an array holding the n-day sigmoid state representation\n",
    "    return np.array([res])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bd8f6",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "798817f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the shape of your training data in order to remond yourself how may features and examples there are in your training set\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851650c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2815813000.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    state = # get the state for the first step\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "keras.config.disable_traceback_filtering() # disable built-in keras loading bars - they make the output difficult to read and monitor\n",
    "\n",
    "\n",
    "l = X_train.shape[0]# track number of examples in dataset (i.e. number of days to train on)\n",
    "\n",
    "# batch size defines how often to run the exp_replay method\n",
    "batch_size = 32\n",
    "\n",
    "#An episode represents a complete pass over the data.\n",
    "episode_count = 2\n",
    "\n",
    "\n",
    "batch_losses = []\n",
    "num_batches_trained = 0\n",
    "\n",
    "for e in range(episode_count + 1):\n",
    "    state = # get the state for the first step\n",
    "    # initialize variables\n",
    "    total_profit = 0\n",
    "    total_winners = 0\n",
    "    total_losers = 0\n",
    "    agent.inventory = []\n",
    "    states_sell = []\n",
    "    states_buy = []\n",
    "    for t in tqdm(range(l), desc==f'Running episode {e}/{episode_count}'):\n",
    "        action = # get the action\n",
    "        next_state = # get the next state\n",
    "        \n",
    "        # initialize reward for the current time step\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1: # buy\n",
    "            # inverse transform to get true buy price in dollars\n",
    "            buy_price = X_train[t, idx_close]\n",
    "            # append the buy price to the inventory\n",
    "            agent.inventory.append(buy_price)\n",
    "            # append the time step to states_buy\n",
    "            states_buy.append(t)\n",
    "            # print the action and price of the action\n",
    "            print(f'Buy: {format_price(buy_price)}')\n",
    "\n",
    "        elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "            bought_price = agent.inventory.pop(0)  \n",
    "            # inverse transform to get true sell price in dollars\n",
    "            sell_price = X_train[t, idx_close]\n",
    "\n",
    "            # define reward as max of profit (close price at time of sell - close price at time of buy) and 0 \n",
    "            trade_profit = sell_price - bought_price\n",
    "            reward = max(trade_profit, 0)\n",
    "            total_profit += trade_profit\n",
    "            if trade_profit >=0:\n",
    "                total_winners += trade_profit\n",
    "            else:\n",
    "                total_losers += trade_profit\n",
    "            states_sell.append(t)\n",
    "            print(f'Sell: {format_price(sell_price)} | Profit: {format_price(trade_profit)}')\n",
    "\n",
    "\n",
    "        # flag for final training iteration\n",
    "        done = True if t == l - 1 else False\n",
    "\n",
    "        # append the details of the state action etc in the memory, to be used by the exp_replay function\n",
    "        state = next_state\n",
    "\n",
    "        # print total profit and plot behaviour of the current episode when the episode is finished\n",
    "        if done:\n",
    "            print('--------------------------------')\n",
    "            print(f'Episode {e}')\n",
    "            print(f'Total Profit: {format_price(total_profit)}')\n",
    "            print(f'Total Winners: {format_price(total_winners)}')\n",
    "            print(f'Total Losers: {format_price(total_losers)}')\n",
    "            print(f'Max Loss: {max(batch_losses[num_batches_trained:len(batch_losses)])}')\n",
    "            print(f'Total Loss: {sum(batch_losses[num_batches_trained:len(batch_losses)])}')\n",
    "            print('--------------------------------')\n",
    "            plot_behavior(X_train[:, idx_close].flatten(), X_train[:, idx_bb_upper].flatten(), X_train[:, idx_bb_lower].flatten(), states_buy, states_sell, total_profit)\n",
    "            plot_losses(batch_losses[num_batches_trained:len(batch_losses)], f'Episode {e} DQN model loss')\n",
    "            num_batches_trained = len(batch_losses)\n",
    "\n",
    "        # when the size of the memory is greater than the batch size, run the exp_replay function on the batch to fit the model and get losses for the batch\n",
    "        # then sum the losses for the batch and append them to the batch_losses list\n",
    "            \n",
    "    if e % 2 == 0:\n",
    "        # save the model every 2 episodes (in case of crash or better training iteration in the middle of training process)\n",
    "        agent.model.save(f'model_ep{e}.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f69aca",
   "metadata": {},
   "source": [
    "### Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc411fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the plot_losses function to plot all batch_losses for the entire training round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb831d",
   "metadata": {},
   "source": [
    "# 5. Test the trained model \n",
    "Finally, we get to test our trained model to see how well it performs in our test set. Using the training loop above, define a method to run our trained model on our X_test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd7885",
   "metadata": {},
   "source": [
    "## Define Parameters\n",
    "Some test parameters are defined for you below. Fill out the missing data. If you need a hint, look up at the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test = len(X_test) - 1\n",
    "state = get_state(X_test, 0, window_size + 1)\n",
    "total_profit = 0\n",
    "done = False\n",
    "states_sell_test = []\n",
    "states_buy_test = []\n",
    "\n",
    "#Get the trained model\n",
    "agent = Agent(window_size, num_features=X_test.shape[1], test_mode=True, model_name=f'model_ep{episode_count}.keras')\n",
    "agent.inventory = []\n",
    "\n",
    "state = # get the first state of the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac67df4",
   "metadata": {},
   "source": [
    "### Run the Test\n",
    "Run the test data through the trained model. Look at the training loop for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(l_test):\n",
    "    action = agent.act(state)\n",
    "    next_state = # get the next state in the test dataset\n",
    "    reward = 0\n",
    "\n",
    "    if action == 1: # buy\n",
    "        # inverse transform to get true buy price in dollars\n",
    "        # append buy prive to inventory\n",
    "        # append time step to states_buy_test\n",
    "        print(f'Buy: {format_price(buy_price)}')\n",
    "\n",
    "    elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "        # get bought price from beginning of inventory\n",
    "        # inverse transform to get true sell price in dollars\n",
    "        # reward is max of profit (close price at time of sell - close price at time of buy)\n",
    "        # update total_test_profit\n",
    "        # append time step to states_sell_test\n",
    "        print(f'Sell: {format_price(sell_price)} | Profit: {format_price(sell_price - bought_price)}')\n",
    "\n",
    "    \n",
    "    if t == l_test - 1:\n",
    "        done = True\n",
    "    # append to memory so we can re-train on 'live' (test) data later    \n",
    "    agent.memory.append((state, action, reward, next_state, done))\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        print('------------------------------------------')\n",
    "        print(f'Total Profit: {format_price(total_profit)}')\n",
    "        print('------------------------------------------')\n",
    "        \n",
    "plot_behavior(X_test[:, idx_close].flatten(),X_test[:, idx_bb_upper].flatten(), X_test[:, idx_bb_lower].flatten(), states_buy_test, states_sell_test, total_profit, train=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
